{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDMD5iLIcQzo",
        "outputId": "c95a98d7-1dbb-4338-ea21-30e38d9d8f16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.0\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       1.0\n",
            "           1       0.00      0.00      0.00       0.0\n",
            "\n",
            "    accuracy                           0.00       1.0\n",
            "   macro avg       0.00      0.00      0.00       1.0\n",
            "weighted avg       0.00      0.00      0.00       1.0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "#question 51\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import csv\n",
        "\n",
        "# Step 1: Generate the \"spam.csv\" file\n",
        "# Sample SMS messages with labels (spam or not spam)\n",
        "sms_data = [\n",
        "    [\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate) T&C's apply 08452810075over18s\", \"spam\"],\n",
        "    [\"Ok lar... Joking wif u oni...\", \"not spam\"],\n",
        "    # Add more SMS messages here as needed\n",
        "]\n",
        "\n",
        "# Write data to CSV file\n",
        "with open('spam.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['text', 'label'])  # Write header\n",
        "    writer.writerows(sms_data)  # Write SMS data\n",
        "\n",
        "# Step 2: Load the data from the \"spam.csv\" file\n",
        "# Load the dataset\n",
        "data = pd.read_csv('spam.csv')\n",
        "\n",
        "# Step 3: Preprocess the data\n",
        "# Convert labels to binary (0 for not spam, 1 for spam)\n",
        "data['label'] = data['label'].map({'not spam': 0, 'spam': 1})\n",
        "\n",
        "# Step 4: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Convert text data into numerical feature vectors using CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "# Step 6: Initialize and train a Multinomial Naive Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Step 7: Predict labels for the test data\n",
        "y_pred = nb_classifier.predict(X_test_vectorized)\n",
        "\n",
        "# Step 8: Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Step 9: Display classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#question 52\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Initialize and train the Gaussian Naive Bayes classifier\n",
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict labels for the test data\n",
        "y_pred = nb_classifier.predict(X_test)\n",
        "\n",
        "# Step 5: Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Step 6: Display classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adUzuxg2cyBW",
        "outputId": "5fd0e299-5b78-4c00-8eb7-a435e65aed72"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#question 53\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)\n",
        "        self.biases_input_hidden = np.zeros((1, self.hidden_size))\n",
        "        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)\n",
        "        self.biases_hidden_output = np.zeros((1, self.output_size))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Forward propagation\n",
        "        self.hidden_output = self.sigmoid(np.dot(X, self.weights_input_hidden) + self.biases_input_hidden)\n",
        "        self.predicted_output = self.sigmoid(np.dot(self.hidden_output, self.weights_hidden_output) + self.biases_hidden_output)\n",
        "        return self.predicted_output\n",
        "\n",
        "    def backward(self, X, y):\n",
        "        # Backpropagation\n",
        "        error = y - self.predicted_output\n",
        "        d_predicted_output = error * self.sigmoid_derivative(self.predicted_output)\n",
        "\n",
        "        error_hidden_layer = d_predicted_output.dot(self.weights_hidden_output.T)\n",
        "        d_hidden_output = error_hidden_layer * self.sigmoid_derivative(self.hidden_output)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights_hidden_output += self.hidden_output.T.dot(d_predicted_output) * self.learning_rate\n",
        "        self.biases_hidden_output += np.sum(d_predicted_output, axis=0, keepdims=True) * self.learning_rate\n",
        "        self.weights_input_hidden += X.T.dot(d_hidden_output) * self.learning_rate\n",
        "        self.biases_input_hidden += np.sum(d_hidden_output, axis=0, keepdims=True) * self.learning_rate\n",
        "\n",
        "    def train(self, X, y, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            # Forward propagation\n",
        "            output = self.forward(X)\n",
        "\n",
        "            # Backpropagation and update weights\n",
        "            self.backward(X, y)\n",
        "\n",
        "            # Print loss every 100 epochs\n",
        "            if epoch % 100 == 0:\n",
        "                loss = np.mean(np.square(y - output))\n",
        "                print(f'Epoch {epoch}, Loss: {loss}')\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Input, output, and hidden layer sizes\n",
        "    input_size = 2\n",
        "    hidden_size = 3\n",
        "    output_size = 1\n",
        "\n",
        "    # Learning rate and number of epochs\n",
        "    learning_rate = 0.1\n",
        "    epochs = 1000\n",
        "\n",
        "    # Sample input and output data\n",
        "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "    y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "    # Initialize and train the neural network\n",
        "    nn = NeuralNetwork(input_size, hidden_size, output_size, learning_rate)\n",
        "    nn.train(X, y, epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlCS0zmZc53V",
        "outputId": "f5444f86-5665-4784-88fa-607aef48cd9a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.2616108924360419\n",
            "Epoch 100, Loss: 0.2488653432944745\n",
            "Epoch 200, Loss: 0.2481218481540455\n",
            "Epoch 300, Loss: 0.2472893766602405\n",
            "Epoch 400, Loss: 0.2463070333238236\n",
            "Epoch 500, Loss: 0.24510116318459482\n",
            "Epoch 600, Loss: 0.24358023025821735\n",
            "Epoch 700, Loss: 0.24162998683559447\n",
            "Epoch 800, Loss: 0.23910942133702576\n",
            "Epoch 900, Loss: 0.23584873879656895\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#question 54\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class WordClassifier:\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)\n",
        "        self.biases_input_hidden = np.zeros((1, self.hidden_size))\n",
        "        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)\n",
        "        self.biases_hidden_output = np.zeros((1, self.output_size))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def one_hot_encode(self, label, num_classes):\n",
        "        # One-hot encode the label\n",
        "        encoded = np.zeros((num_classes))\n",
        "        encoded[label] = 1\n",
        "        return encoded\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Forward propagation\n",
        "        self.hidden_output = self.sigmoid(np.dot(X, self.weights_input_hidden) + self.biases_input_hidden)\n",
        "        self.predicted_output = self.sigmoid(np.dot(self.hidden_output, self.weights_hidden_output) + self.biases_hidden_output)\n",
        "        return self.predicted_output\n",
        "\n",
        "    def backward(self, X, y):\n",
        "        # Backpropagation\n",
        "        error = y - self.predicted_output\n",
        "        d_predicted_output = error * self.sigmoid_derivative(self.predicted_output)\n",
        "\n",
        "        error_hidden_layer = d_predicted_output.dot(self.weights_hidden_output.T)\n",
        "        d_hidden_output = error_hidden_layer * self.sigmoid_derivative(self.hidden_output)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights_hidden_output += self.hidden_output.T.dot(d_predicted_output) * self.learning_rate\n",
        "        self.biases_hidden_output += np.sum(d_predicted_output, axis=0, keepdims=True) * self.learning_rate\n",
        "        self.weights_input_hidden += X.T.dot(d_hidden_output) * self.learning_rate\n",
        "        self.biases_input_hidden += np.sum(d_hidden_output, axis=0, keepdims=True) * self.learning_rate\n",
        "\n",
        "    def train(self, X, y, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            # Forward propagation\n",
        "            output = self.forward(X)\n",
        "\n",
        "            # Backpropagation and update weights\n",
        "            self.backward(X, y)\n",
        "\n",
        "            # Print loss every 100 epochs\n",
        "            if epoch % 100 == 0:\n",
        "                loss = np.mean(np.square(y - output))\n",
        "                print(f'Epoch {epoch}, Loss: {loss}')\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Sample vocabulary and word labels\n",
        "    vocabulary = ['hello', 'world', 'python', 'programming']\n",
        "    word_labels = {word: i for i, word in enumerate(vocabulary)}\n",
        "\n",
        "    # Input size, hidden size, output size, learning rate, and number of epochs\n",
        "    input_size = len(vocabulary)\n",
        "    hidden_size = 5\n",
        "    output_size = len(vocabulary)\n",
        "    learning_rate = 0.1\n",
        "    epochs = 1000\n",
        "\n",
        "    # Sample input and output data\n",
        "    X = np.array([WordClassifier(input_size, hidden_size, output_size, learning_rate).one_hot_encode(word_labels[word], len(vocabulary)) for word in vocabulary])\n",
        "    y = np.array([WordClassifier(input_size, hidden_size, output_size, learning_rate).one_hot_encode(word_labels[word], len(vocabulary)) for word in vocabulary])\n",
        "\n",
        "    # Initialize and train the word classifier\n",
        "    word_classifier = WordClassifier(input_size, hidden_size, output_size, learning_rate)\n",
        "    word_classifier.train(X, y, epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNanSlG4dDp6",
        "outputId": "51cd500e-7444-4063-9d87-0becc6a9fe52"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.281504484392445\n",
            "Epoch 100, Loss: 0.1353629939439887\n",
            "Epoch 200, Loss: 0.09500992775532002\n",
            "Epoch 300, Loss: 0.06098796755901016\n",
            "Epoch 400, Loss: 0.03890708847085004\n",
            "Epoch 500, Loss: 0.026193249760009196\n",
            "Epoch 600, Loss: 0.018863421480750136\n",
            "Epoch 700, Loss: 0.014389708969666546\n",
            "Epoch 800, Loss: 0.011473765670004365\n",
            "Epoch 900, Loss: 0.009460698851133363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlxtend\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6QpbWLwdqVF",
        "outputId": "5c9d3d56-7886-4aa2-f5be-9d03ccb78740"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.10/dist-packages (0.22.0)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from mlxtend) (1.11.4)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from mlxtend) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from mlxtend) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from mlxtend) (1.2.2)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from mlxtend) (3.7.1)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from mlxtend) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from mlxtend) (67.7.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->mlxtend) (2023.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->mlxtend) (3.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#question 55\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori\n",
        "from mlxtend.frequent_patterns import association_rules\n",
        "\n",
        "# Sample dataset (replace this with your dataset)\n",
        "data = {\n",
        "    'Transaction': ['T1', 'T1', 'T2', 'T3', 'T3', 'T3', 'T4', 'T4', 'T4', 'T5'],\n",
        "    'Item': ['A', 'B', 'A', 'A', 'B', 'C', 'A', 'B', 'D', 'A']\n",
        "}\n",
        "\n",
        "# Convert data to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Perform one-hot encoding\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit_transform(df.groupby(['Transaction'])['Item'].apply(list))\n",
        "df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "\n",
        "# Apply Apriori algorithm to find frequent itemsets\n",
        "frequent_itemsets = apriori(df_encoded, min_support=0.2, use_colnames=True)\n",
        "\n",
        "# Generate association rules\n",
        "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
        "\n",
        "# Print the rules\n",
        "print(rules)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00h5mtHrd6m-",
        "outputId": "57d1e92a-af4d-4f0d-aabf-bbafb5fd15bb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   antecedents consequents  antecedent support  consequent support  support  \\\n",
            "0          (B)         (A)                 0.6                 1.0      0.6   \n",
            "1          (C)         (A)                 0.2                 1.0      0.2   \n",
            "2          (D)         (A)                 0.2                 1.0      0.2   \n",
            "3          (C)         (B)                 0.2                 0.6      0.2   \n",
            "4          (D)         (B)                 0.2                 0.6      0.2   \n",
            "5       (B, C)         (A)                 0.2                 1.0      0.2   \n",
            "6       (A, C)         (B)                 0.2                 0.6      0.2   \n",
            "7          (C)      (B, A)                 0.2                 0.6      0.2   \n",
            "8       (B, D)         (A)                 0.2                 1.0      0.2   \n",
            "9       (D, A)         (B)                 0.2                 0.6      0.2   \n",
            "10         (D)      (B, A)                 0.2                 0.6      0.2   \n",
            "\n",
            "    confidence      lift  leverage  conviction  zhangs_metric  \n",
            "0          1.0  1.000000      0.00         inf            0.0  \n",
            "1          1.0  1.000000      0.00         inf            0.0  \n",
            "2          1.0  1.000000      0.00         inf            0.0  \n",
            "3          1.0  1.666667      0.08         inf            0.5  \n",
            "4          1.0  1.666667      0.08         inf            0.5  \n",
            "5          1.0  1.000000      0.00         inf            0.0  \n",
            "6          1.0  1.666667      0.08         inf            0.5  \n",
            "7          1.0  1.666667      0.08         inf            0.5  \n",
            "8          1.0  1.000000      0.00         inf            0.0  \n",
            "9          1.0  1.666667      0.08         inf            0.5  \n",
            "10         1.0  1.666667      0.08         inf            0.5  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    }
  ]
}